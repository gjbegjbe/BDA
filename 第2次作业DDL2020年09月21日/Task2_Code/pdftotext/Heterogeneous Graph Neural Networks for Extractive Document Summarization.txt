Heterogeneous Graph Neural Networks for Extractive
Document Summarization
Danqing Wang‚àó, Pengfei Liu‚àó, Yining Zheng, Xipeng Qiu‚Ä†, Xuanjing Huang
Shanghai Key Laboratory of Intelligent Information Processing, Fudan University
School of Computer Science, Fudan University
825 Zhangheng Road, Shanghai, China
{dqwang18,pfliu14,ynzheng19,xpqiu,xjhuang}@fudan.edu.cn

Abstract
As a crucial step in extractive document summarization, learning cross-sentence relations
has been explored by a plethora of approaches.
An intuitive way is to put them in the graphbased neural network, which has a more complex structure for capturing inter-sentence relationships. In this paper, we present a heterogeneous graph-based neural network for extractive summarization (H ETER SUMG RAPH),
which contains semantic nodes of different
granularity levels apart from sentences. These
additional nodes act as the intermediary between sentences and enrich the cross-sentence
relations. Besides, our graph structure is
flexible in natural extension from a singledocument setting to multi-document via introducing document nodes. To our knowledge,
we are the first one to introduce different types
of nodes into graph-based neural networks for
extractive document summarization and perform a comprehensive qualitative analysis to
investigate their benefits. The code will be released on Github1 .

1

Introduction

Extractive document summarization aims to extract
relevant sentences from the original documents and
reorganize them as the summary. Recent years
have seen a resounding success in the use of deep
neural networks on this task (Cheng and Lapata,
2016; Narayan et al., 2018; Arumae and Liu, 2018;
Zhong et al., 2019a; Liu and Lapata, 2019b). These
existing models mainly follow the encoder-decoder
framework in which each sentence will be encoded
by neural components with different forms.
To effectively extract the summary-worthy sentences from a document, a core step is to model
‚àó

These two authors contributed equally.
Corresponding author.
1
https://github.com/brxx122/
HeterSUMGraph
‚Ä†

the cross-sentence relations. Most current models capture cross-sentence relations with recurrent
neural networks (RNNs) (Cheng and Lapata, 2016;
Nallapati et al., 2017; Zhou et al., 2018). However, RNNs-based models are usually hard to capture sentence-level long-distance dependency, especially in the case of the long document or multidocuments. One more intuitive way is to model
the relations of sentences using the graph structure. Nevertheless, it is challenging to find an effective graph structure for summarization. Efforts
have been made in various ways. Early traditional
work makes use of inter-sentence cosine similarity to build the connectivity graph like LexRank
(Erkan and Radev, 2004) and TextRank (Mihalcea
and Tarau, 2004). Recently, some works account
for discourse inter-sentential relationships when
building summarization graphs, such as the Approximate Discourse Graph (ADG) with sentence
personalization features (Yasunaga et al., 2017) and
Rhetorical Structure Theory (RST) graph (Xu et al.,
2019). However, they usually rely on external tools
and need to take account of the error propagation
problem. A more straightforward way is to create
a sentence-level fully-connected graph. To some
extent, the Transformer encoder (Vaswani et al.,
2017) used in recent work(Zhong et al., 2019a;
Liu and Lapata, 2019b) can be classified into this
type, which learns the pairwise interaction between
sentences. Despite their success, how to construct
an effective graph structure for summarization remains an open question.
In this paper, we propose a heterogeneous graph
network for extractive summarization. Instead of
solely building graphs on sentence-level nodes, we
introduce more semantic units as additional nodes
in the graph to enrich the relationships between
sentences. These additional nodes act as the intermediary that connects sentences. Namely, each
additional node can be viewed as a special rela-

6209
Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6209‚Äì6219
July 5 - 10, 2020. c 2020 Association for Computational Linguistics

tionship between sentences containing it. During
the massage passing over the heterogeneous graph,
these additional nodes will be iteratively updated
as well as sentence nodes.
Although more advanced features can be used
(e.g., entities or topics), for simplicity, we use
words as the semantic units in this paper. Each
sentence is connected to its contained words. There
are no direct edges for all the sentence pairs and
word pairs. The constructed heterogeneous wordsentence graph has the following advantages: (a)
Different sentences can interact with each other in
consideration of the explicit overlapping word information. (b) The word nodes can also aggregate
information from sentences and get updated. Unlike ours, existing models usually keep the words
unchanged as the embedding layer. (c) Different granularities of information can be fully used
through multiple message passing processes. (d)
Our heterogeneous graph network is expandable
for more types of nodes. For example, we can introduce document nodes for multi-document summarization.
We highlight our contributions as follows:
(1) To our knowledge, we are the first one to construct a heterogeneous graph network for extractive
document summarization to model the relations between sentences, which contains not only sentence
nodes but also other semantic units. Although we
just use word nodes in this paper, more superior
semantic units (e.g. entities) can be incorporated.
(2) Our proposed framework is very flexible
in extension that can be easily adapt from singledocument to multi-document summarization tasks.
(3) Our model can outperform all existing competitors on three benchmark datasets without the
pre-trained language models2 . Ablation studies and
qualitative analysis show the effectiveness of our
models.

2

Related Work

Extractive Document Summarization With
the development of neural networks, great progress
has been made in extractive document summarization. Most of them focus on the encoderdecoder framework and use recurrent neural networks (Cheng and Lapata, 2016; Nallapati et al.,
2017; Zhou et al., 2018) or Transformer encoders
2
Since our proposed model is orthogonal to the methods
that using pre-trained models, we believe our model can be
further boosted by taking the pre-trained models to initialize
the node representations, which we reserve for the future.

(Zhong et al., 2019b; Wang et al., 2019a) for the
sentential encoding. Recently, pre-trained language
models are also applied in summarization for contextual word representations (Zhong et al., 2019a;
Liu and Lapata, 2019b; Xu et al., 2019; Zhong
et al., 2020).
Another intuitive structure for extractive summarization is the graph, which can better utilize the
statistical or linguistic information between sentences. Early works focus on document graphs
constructed with the content similarity among sentences, like LexRank (Erkan and Radev, 2004) and
TextRank (Mihalcea and Tarau, 2004). Some recent works aim to incorporate a relational priori
into the encoder by graph neural networks (GNNs)
(Yasunaga et al., 2017; Xu et al., 2019). Methodologically, these works only use one type of nodes,
which formulate each document as a homogeneous
graph.
Heterogeneous Graph for NLP Graph neural
networks and their associated learning methods
(i.e. message passing (Gilmer et al., 2017), selfattention (Velickovic et al., 2017)) are originally
designed for the homogeneous graph where the
whole graph shares the same type of nodes. However, the graph in the real-world application usually comes with multiple types of nodes (Shi et al.,
2016), namely the heterogeneous graph. To model
these structures, recent works have made preliminary exploration. Tu et al. (2019) introduced a
heterogeneous graph neural network to encode documents, entities and candidates together for multihop reading comprehension. Linmei et al. (2019)
focused on semi-supervised short text classification and constructed a topic-entity heterogeneous
neural graph.
For summarization, Wei (2012) proposes a heterogeneous graph consisting of topic, word and
sentence nodes and uses the markov chain model
for the iterative update. Wang et al. (2019b) modify
TextRank for their graph with keywords and sentences and thus put forward HeteroRank. Inspired
by the success of the heterogeneous graph-based
neural network on other NLP tasks, we introduce
it to extractive text summarization to learn a better
node representation.

3

Methodology

Given a document D = {s1 , ¬∑ ¬∑ ¬∑ , sn } with n sentences, we can formulate extractive summarization
as a sequence labeling task as (Narayan et al., 2018;

6210

our framework, other types of supernodes (such as
paragraphs) can also be introduced and the only
difference lies in the graph structure.

Sentence
Selector

Graph Layer

3.1

ùë§3

ùë§1
ùë†2

ùë§2

Word Node

Edge Feature

Word
Encoder

TF-IDF

Word
Word

Document as a Heterogeneous Graph

ùë†1

Sentence Node

BiLSTM
CNN
Encoder
Sentence
Sentence

Figure 1: Model Overview. The framework consists of
three major modules: graph initializers, the heterogeneous graph layer and the sentence selector. Green circles and blue boxes represent word and sentence nodes
respectively. Orange solid lines denote the edge feature
(TF-IDF) between word and sentence nodes and the
thicknesses indicate the weight. The representations of
sentence nodes will be finally used for summary selection.

Liu and Lapata, 2019b). Our goal is to predict a
sequence of labels y1 , ¬∑ ¬∑ ¬∑ , yn (yi ‚àà {0, 1}) for sentences, where yi = 1 represents the i-th sentence
should be included in the summaries. The ground
truth labels, which we call ORACLE, is extracted
using the greedy approach introduced by Nallapati
et al. (2016) with the automatic metrics ROUGE
(Lin and Hovy, 2003).
Generally speaking, our heterogeneous summarization graph consists of two types of nodes: basic
semantic nodes (e.g. words, concepts, etc.) as relay
nodes and other units of discourse (e.g. phrases,
sentences, documents, etc.) as supernodes. Each
supernode connects with basic nodes contained in
it and takes the importance of the relation as their
edge feature. Thus, high-level discourse nodes can
establish relationships between each other via basic
nodes.
In this paper, we use words as the basic semantic nodes for simplicity. H ETER SUMG RAPH in
Section 3.1 is a special case which only contains
one type of supernodes (sentences) for classification, while H ETER D OC SUMG RAPH in Section
3.5 use two (documents and sentences). Based on

Given a graph G = {V, E}, where V stands
for a node set and E represents edges between
nodes, our undirected heterogeneous graph can
be formally defined as V = Vw ‚à™ Vs and E =
{e11 , ¬∑ ¬∑ ¬∑ , emn }. Here, Vw = {w1 , ¬∑ ¬∑ ¬∑ , wm } denotes m unique words of the document and Vs =
{s1 , ¬∑ ¬∑ ¬∑ , sn } corresponds to the n sentences in the
document. E is a real-value edge weight matrix
and eij 6= 0 (i ‚àà {1, ¬∑ ¬∑ ¬∑ , m}, j ‚àà {1, ¬∑ ¬∑ ¬∑ , n})
indicates the j-th sentence contains the i-th word.
Figure 1 presents the overview of our model,
which mainly consists of three parts: graph initializers for nodes and edges, the heterogeneous
graph layer and the sentence selector. The initializers first create nodes and edges and encode
them for the document graph. Then the heterogeneous graph updates these node representations by
iteratively passing messages between word and sentence nodes via Graph Attention Network (GAT)
(Velickovic et al., 2017). Finally, the representations of sentence nodes are extracted to predict
labels for summaries.
3.2

Graph Initializers

Let Xw ‚àà Rm√ódw and Xs ‚àà Rn√óds represent the
input feature matrix of word and sentence nodes respectively, where dw is the dimension of the word
embedding and ds is the dimension of each sentence representation vector. Specifically, we first
use Convolutional Neural Networks (CNN) (LeCun et al., 1998) with different kernel sizes to capture the local n-gram feature for each sentence lj
and then use the bidirectional Long Short-Term
Memory (BiLSTM) (Hochreiter and Schmidhuber,
1997) layer to get the sentence-level feature gj .
The concatenation of the CNN local feature and
the BiLSTM global feature is used as the sentence
node feature Xsj = [lj ; gj ].
To further include information about the importance of relationships between word and sentence nodes, we infuse TF-IDF values in the edge
weights. The term frequency (TF) is the number
of times wi occurs in sj and the inverse document
frequency (IDF) is made as the inverse function of
the out-degree of wi .

6211

3.3

Heterogeneous Graph Layer

ùë§3

Given a constructed graph G with node features
Xw ‚à™ Xs and edge features E, we use graph attention networks (Velickovic et al., 2017) to update
the representations of our semantic nodes.
We refer to hi ‚àà Rdh , i ‚àà {1, ¬∑ ¬∑ ¬∑ , (m + n)}
as the hidden states of input nodes and the graph
attention (GAT) layer is designed as follows:
zij = LeakyReLU (Wa [Wq hi ; Wk hj ]) ,
exp(zij )
,
Œ±ij = P
l‚ààNi exp(zil )
X
Œ±ij Wv hj ),
ui = œÉ(

(1)
(2)
(3)

j‚ààNi

where Wa , Wq , Wk , Wv are trainable weights
and Œ±ij is the attention weight between hi and hj .
The multi-head attention can be denoted as:
Ô£´
Ô£∂
X
k
Ô£≠
ui = kK
Œ±ij
W k hi Ô£∏ .
(4)
k=1 œÉ
j‚ààNi

Besides, we also add a residual connection to
avoid gradient vanishing after several iterations.
Therefore, the final output can be represented as:
h0i = ui + hi .

zij = LeakyReLU (Wa [Wq hi ; Wk hj ; eij ]) .

(6)

After each graph attention layer, we introduce a
position-wise feed-forward (FFN) layer consisting
of two linear transformations just as Transformer
(Vaswani et al., 2017).
Iterative updating To pass messages between
word and sentence nodes, we define the information
propagation as Figure 2. Specifically, after the
initialization, we update sentence nodes with their
neighbor word nodes via the above GAT and FFN
layer:
U1s‚Üêw = GAT(H0s , H0w , H0w ),

H1s = FFN U1s‚Üêw + H0s ,

ùë§1

(7)
(8)

where H1w = H0w = Xw , H0s = Xs and U1s‚Üêw ‚àà
Rm√ódh . GAT(H0s , H0w , H0w ) denotes that H0s is
used as the attention query and H0w is used as the
key and value.

ùë†2
ùë§2

ùë§3

ùë†1

ùë§1

ùë†2

ùë§2
(b) Update ùë§1

(a) Update ùë†1

Figure 2: The detailed update process of word and sentence nodes in Heterogeneous Graph Layer. Green and
blue nodes are word and sentence nodes involved in
this turn. Orange edges indicate the current information flow direction. First, for sentence s1 , word w1 and
w3 are used to aggregate word-level information in (a).
Next, w1 is updated by the new representation of s1 and
s2 in (b), which are the sentences it occurs. See Section
3.3 for details on the notation.

After that, we obtain new representations for
word nodes using the updated sentence nods and
further update sentence nodes iteratively. Each
iteration contains a sentence-to-word and a wordto-sentence update process. For the t-th iteration,
the process can be represented as:

(5)

We further modify the GAT layer to infuse
the scalar edge weights eij , which are mapped
to the multi-dimensional embedding space eij ‚àà
Rmn√óde . Thus, Equal 1 is modified as follows:

ùë†1

t
t
t
Ut+1
w‚Üês = GAT(Hw , Hs , Hs ),
t 
Ht+1
= FFN Ut+1
w
w‚Üês + Hw ,

(10)

Ut+1
s‚Üêw

(11)

Ht+1
s

=

t+1
GAT(Hts , Ht+1
w , Hw ),

= FFN

Ut+1
s‚Üêw

+


Hts .

(9)

(12)

As Figure 2 shows, word nodes can aggregate the
document-level information from sentences. For
example, the high degree of a word node indicates
the word occurs in many sentences and is likely
to be the keyword of the document. Regarding
sentence nodes, the one with more important words
tends to be selected as the summary.
3.4

Sentence Selector

Finally, we need to extract sentence nodes included
in the summary from the heterogeneous graph.
Therefore, we do node classification for sentences
and cross-entropy loss is used as the training objective for the whole system.
Trigram blocking Following Paulus et al. (2017)
and Liu and Lapata (2019b), we use Trigram Blocking for decoding, which is simple but powerful version of Maximal Marginal Relevance (Carbonell
and Goldstein, 1998). Specifically, we rank sentences by their scores and discard those which have
trigram overlappings with their predecessors.

6212

The differences lie in the initialization, where the
document node takes the mean-pooling of its sentence node features as its initial state. During the
sentence selection, the sentence nodes are concatenated with the corresponding document representations to obtain the final scores for multi-document
summarization.

ùë†!!
ùë§!
ùëë!

ùëë"

ùë§"
ùë§$
ùë§#

ùë†!"
ùë†"!
ùë†""

4
Figure 3: Graph structure of H ETER D OC SUMG RAPH
for multi-document summarization (corresponding to
the Graph Layer part of Figure 1). Green, blue and
orange boxes represent word, sentence and document
nodes respectively. d1 consists of s11 and s12 while d2
contains s21 and s22 . As a relay node, the relation of
document-document, sentence-sentence, and sentencedocument can be built through the common word nodes.
For example, sentence s11 , s12 and s21 share the same
word w1 , which connects them across documents.

3.5

Multi-document Summarization

For multi-document summarization, the documentlevel relation is crucial for better understanding the
core topic and most important content of this cluster. However, most existing neural models ignore
this hierarchical structure and concatenate documents to a single flat sequence(Liu et al., 2018;
Fabbri et al., 2019). Others try to model this relation by attention-based full-connected graph or take
advantage of similarity or discourse relations(Liu
and Lapata, 2019a).
Our framework can establish the document-level
relationship in the same way as the sentence-level
by just adding supernodes for documents(as Figure 3), which means it can be easily adapted from
single-document to multi-document summarization. The heterogeneous graph is then extended
to three types of nodes: V = Vw ‚à™ Vs ‚à™ Vd
and Vd = {d1 , ¬∑ ¬∑ ¬∑ , dl } and l is the number of
source documents. We name it as H ETER D OC SUMG RAPH.
As we can see in Figure 3, word nodes become
the bridges between sentences and documents. Sentences containing the same words connect with
each other regardless of their distance across documents, while documents establish relationships
based on their similar contents.
Document nodes can be viewed as a special type
of sentence nodes: a document node connects with
contained word nodes and the TF-IDF value is used
as the edge weight. Besides, document nodes also
share the same update process as sentence nodes.

Experiment

We evaluate our models both on single- and multidocument summarization tasks. Below, we start
our experiment with the description of the datasets.
4.1

Datasets

CNN/DailyMail The CNN/DailyMail question
answering dataset (Hermann et al., 2015; Nallapati et al., 2016) is the most widely used
benchmark dataset for single-document summarization. The standard dataset split contains
287,227/13,368/11,490 examples for training, validation, and test. For the data prepossessing, we
follow Liu and Lapata (2019b), which use the nonanonymized version as See et al. (2017), to get
ground-truth labels.
NYT50 NYT50 is also a single-document summarization dataset, which was collected from New
York Times Annotated Corpus (Sandhaus, 2008)
and preprocessed by Durrett et al. (2016). It contains 110,540 articles with summaries and is split
into 100,834 and 9706 for training and test. Following Durrett et al. (2016), we use the last 4,000
examples from the training set as validation and
filter test examples to 3,452.
Multi-News The Multi-News dataset is a largescale multi-document summarization introduced
by Fabbri et al. (2019). It contains 56,216 articlessummary pairs and each example consists of 2-10
source documents and a human-written summary.
Following their experimental settings, we split the
dataset into 44,972/5,622/5,622 for training, validation and test examples and truncate input articles
to 500 tokens.
4.2

Settings and Hyper-parameters

For both single-document and multi-document
summarization, we limit the vocabulary to 50,000
and initialize tokens with 300-dimensional GloVe
embeddings (Pennington et al., 2014). We filter
stop words and punctuations when creating word

6213

nodes and truncate the input document to a maximum length of 50 sentences. To get rid of the
noisy common words, we further remove 10% of
the vocabulary with low TF-IDF values over the
whole dataset. We initialize sentence nodes with
ds = 128 and edge features eij in GATe with
de = 50. Each GAT layer is 8 heads and the hidden
size is dh = 64, while the inner hidden size of FFN
layers is 512.
During training, we use a batch size of 32 and
apply Adam optimizer (Kingma and Ba, 2014) with
a learning rate 5e-4. An early stop is performed
when valid loss does not descent for three continuous epochs. We select the number of iterations
t = 1 based on the performance on the validation
set.3 For decoding, we select top-3 sentences for
CNN/DailyMail and NYT50 datasets and top-9 for
Multi-New according to the average length of their
human-written summaries.

Model

Models for Comparison

Ext-BiLSTM Extractive summarizer with BiLSTM encoder learns the cross-sentence relation by
regarding a document as a sequence of sentences.
For simplification, we directly take out the initialization of sentence nodes for classification, which
includes a CNN encoder for the word level and 2layer BiLSTM for sentence level. This model can
also be viewed as an ablation study of our H ETER SUMG RAPH on the updating of sentence nodes.
Ext-Transformer Extractive summarizers with
Transformer encoder learn the pairwise interaction
(Vaswani et al., 2017) between sentences in a purely
data-driven way with a fully connected priori. Following (Liu and Lapata, 2019b), we implement a
Transformer-based extractor as a baseline, which
contains the same encoder for words followed by
12 Transformer encoder layers for sentences. ExtTransformer can be regarded as the sentence-level
fully connected graph.
H ETER SUMG RAPH Our heterogeneous summarization graph model relations between sentences based on their common words, which can be
denoted as sentence-word-sentence relationships.
H ETER SUMG RAPH directly selects sentences for
the summary by node classification, while H ETER SUMG RAPH with trigram blocking further utilizes
the n-gram blocking to reduce redundancy.
3
The detailed experimental results are attached in the Appendix Section.

R-2

R-L

L EAD -3 (See et al., 2017)
O RACLE (Liu and Lapata, 2019b)

40.34 17.70 36.57
52.59 31.24 48.87

REFRESH (Narayan et al., 2018)
LATENT (Zhang et al., 2018)
BanditSum (Dong et al., 2018)
NeuSUM (Zhou et al., 2018)
JECS (Xu and Durrett, 2019)
LSTM+PN (Zhong et al., 2019a)
HER w/o Policy (Luo et al., 2019)
HER w Policy (Luo et al., 2019)

40.00
41.05
41.50
41.59
41.70
41.85
41.70
42.30

18.20
18.77
18.70
19.01
18.50
18.93
18.30
18.90

36.60
37.54
37.60
37.98
37.90
38.13
37.10
37.60

Ext-BiLSTM
Ext-Transformer
HSG
HSG + Tri-Blocking

41.59
41.33
42.31
42.95

19.03
18.83
19.51
19.76

38.04
37.65
38.74
39.23

Table 1: Performance (Rouge) of our proposed models against recently released summarization systems on
CNN/DailyMail.

5
5.1

4.3

R-1

Results and Analysis
Single-document Summarization

We evaluate our single-document model on
CNN/DailyMail and NYT50 and report the unigram, bigram and longest common subsequence
overlap with reference summaries by R-1, R-2 and
R-L. Due to the limited computational resource, we
don‚Äôt apply pre-trained contextualized encoder (i.e.
BERT (Devlin et al., 2018)) to our models, which
we will regard as our future work. Therefore, here,
we only compare with models without BERT for
the sake of fairness.
Results on CNN/DailyMail Table 1 shows the
results on CNN/DailyMail. The first part is the
L EAD -3 baseline and ORACLE upper bound, while
the second part includes other summarization models.
We present our models (described in Section
4.3) in the third part. Compared with ExtBiLSTM, our heterogeneous graphs achieve more
than 0.6/0.51/0.7 improvements on R-1, R-2 and
R-L, which indicates the cross-sentence relationships learned by our sentence-word-sentence structure is more powerful than the sequential structure. Besides, Our models also outperform ExtTransformer based on fully connected relationships.
This demonstrates that our graph structures effectively prune unnecessary connections between sentences and thus improve the performance of sentence node classification.
Compared with the second block of Figure 1, we
observe that H ETER SUMG RAPH outperforms all
previous non-BERT-based summarization systems

6214

and trigram blocking leads to a great improvement
on all ROUGE metrics. Among them, HER (Luo
et al., 2019) is a comparable competitor to our H ETER SUMG RAPH , which formulated the extractive
summarization task as a contextual-bandit problem
and solved it with reinforcement learning. Since the
reinforcement learning and our trigram blocking
plays a similar role in reorganizing sentences into
a summary (Zhong et al., 2019a), we additionally
compare HER without policy gradient with H ETER SUMG RAPH. Our H ETER SUMG RAPH achieve
0.61 improvements on R-1 over HER without policy for sentence scoring, and H ETER SUMG RAPH
with trigram blocking outperforms by 0.65 over
HER for the reorganized summaries.
Model

R-1

R-2

R-L

First sentence (Durrett et al., 2016)
First k words (Durrett et al., 2016)
L EAD -3
O RACLE

28.60
35.70
38.99
60.54

17.30
21.60
18.74 35.35
40.75 57.22

COMPRESS (Durrett et al., 2016)
SUMO (Liu et al., 2019)
PG* (See et al., 2017)
DRM (Paulus et al., 2017)

42.20
42.30
43.71
42.94

24.90
22.70 38.60
26.40
26.02
-

Ext-BiLSTM
Ext-Transformer
HSG
HSG + Tri-Blocking

46.32
45.07
46.89
46.57

25.84
24.72
26.26
25.94

42.16
40.85
42.58
42.25

Table 2: Limited-length ROUGE Recall on NYT50 test
set. The results of models with * are copied from Liu
and Lapata (2019b) and ‚Äô-‚Äô means that the original paper did not report the result.

Results on NYT50 Results on NYT50 are summarized in Table 2. Note that we use limited-length
ROUGE recall as Durrett et al. (2016), where the
selected sentences are truncated to the length of
the human-written summaries and the recall scores
are used instead of F1. The first two lines are baselines given by Durrett et al. (2016) and the next two
lines are our baselines for extractive summarization.
The second and third part report the performance
of other non-BERT-based works and our models
respectively.
Again, we observe that our cross-sentence relationship modeling performs better than BiLSTM
and Transformer. Our models also have strong advantages over other non-BERT-based approaches
on NYT50. Meanwhile, we find trigram block
doesn‚Äôt work as well as shown on CNN/DailyMail,
and we attribute the reason to the special formation

of summaries of CNN/DailyMail dataset.

4

Ablation on CNN/DailyMail In order to better
understand the contribution of different modules
to the performance, we conduct ablation study using our proposed H ETER SUMG RAPH model on
CNN/DailyMail dataset. First, we remove the filtering mechanism for low TF-IDF words and the
edge weights respectively. We also remove residual
connections between GAT layers. As a compensation, we concatenate the initial sentence feature
after updating messages from nearby word nodes
in Equal 8:

H1s = FFN [U1s‚Üêw ; H0s ] .

(13)

Furthermore, we make iteration number t = 0,
which deletes the word updating and use the sentence representation H1s for classification. Finally,
we remove the BiLSTM layer in the initialization
of sentence nodes.
As Table 3 shows, the removal of low TF-IDF
words leads to increases on R-1 and R-L but drops
on R-2. We suspect that filtering noisy words
enable the model to better focus on useful word
nodes, at the cost of losing some bigram information. The residual connection plays an important
role in the combination of the original representation and the updating message from another type
of nodes, which cannot be replaced by the concatenation. Besides, the introduction of edge features,
word update and BiLSTM initialization for sentences also show their effectiveness.
5.2

Multi-document Summarization

We first take the concatenation of the First-k sentences from each source document as the baseline
and use the codes and model outputs5 released by
Fabbri et al. (2019) for other models.
To explore the adaptability of our model to multidocument summarization, we concatenate multisource documents to a single mega-document and
apply H ETER SUMG RAPH as the baseline. For
comparison, we extend H ETER SUMG RAPH to
multi-document settings H ETER D OC SUMG RAPH
4
Nallapati et al. (2016) concatenate summary bullets,
which are written for different parts of the article and have
few overlaps with each other, as a multi-sentence summary.
However, when human write summaries for the whole article
(such as NYT50 and Multi-News), they will use key phrases
repeatedly. This means roughly removing sentences by n-gram
overlaps will lead to loss of important information.
5
https://github.com/Alex-Fabbri/ Multi-News

6215

R-2

R-L

42.31
42.24
42.14
41.59
41.59
41.70
41.70

19.51
19.56
19.41
19.08
19.03
19.16
19.09

38.74
38.68
38.60
38.05
38.04
38.15
38.13

BiLSTM
HSG
35

as described in Section 3.5. Our results are presented in Table 4.
Specifically, we observe that both of our H ETER SUMG RAPH and H ETER D OC SUMG RAPH outperform previous methods while H ETER D OC SUMG RAPH achieves better performance improvements. This demonstrates the introduction of
document nodes can better model the documentdocument relationships and is beneficial for multidocument summarization. As mentioned above,
trigram blocking does not work for the Multi-News
dataset, since summaries are written as a whole
instead of the concatenations of summary bullets
for each source document.

First-1
First-2
First-3
O RACLE

R-1 R-2 R-L
25.44 7.06 22.12
35.70 10.28 31.71
40.21 12.13 37.13
52.32 22.23 47.93

LexRank* (Erkan and Radev, 2004)
41.77 13.81 37.87
TextRank* (Mihalcea and Tarau, 2004) 41.95 13.86 38.07
MMR* (Carbonell and Goldstein, 1998) 44.72 14.92 40.77
PG‚Ä† (Lebanoff et al., 2018)
44.55 15.54 40.75
BottomUp‚Ä† (Gehrmann et al., 2018)
45.27 15.32 41.38
Hi-MAP‚Ä† (Fabbri et al., 2019)
45.21 16.29 41.39
HSG
HSG + Tri-Blocking
HDSG
HDSG + Tri-Blocking

45.66 16.22 41.80
44.92 15.59 40.89
46.05 16.35 42.08
45.55 15.78 41.29

Table 4: Results on the test set of Multi-News. We
reproduce models with ‚Äò*‚Äô via the released code and
directly use the outputs of ‚Ä† provided by Fabbri et al.
(2019) for evaluation.

5.3

0.6
30

Table 3: Ablation studies on CNN/DailyMail test set.
We remove various modules and explore their influence
on our model. ‚Äô-‚Äô means we remove the module from
the original H ETER SUMG RAPH. Note that H ETER SUMG RAPH without the updating of sentence nodes
is actually the Ext-BiLSTM model described in Section
4.3.

Model

0.8

‚àÜRÃÉ

HSG
- filter words
- edge feature
- residual connection
- sentence update
- word update
- BiLSTM

R-1

RÃÉ

Model

Qualitative Analysis

We further design several experiments to probe into
how our H ETER SUMG RAPH and H ETER D OC -

(0, 1.25) (1.25, 1.5)(1.5, 1.75)(1.75, 2.0) (2.0, ‚àû)

0.4

Average degree of word nodes

Figure 4: Relationships between the average degree of
word nodes of the document (x-axis) and RÃÉ, which is
the mean of R-1, R-2 and R-L (lines for left y-axis),
and between ‚àÜRÃÉ, which is the delta RÃÉ of H ETER SUMG RAPH and Ext-BiLSTM (histograms for right y-axis).

SUMG RAPH help the single- and multi-document
summarization.
Degree of word nodes In H ETER SUMG RAPH,
the degree of a word node indicates its occurrence
across sentences and thus can measure the redundancy of the document to some extent. Meanwhile,
words with a high degree can aggregate information from multiple sentences, which means that
they can benefit more from the iteration process.
Therefore, it is important to explore the influence
of the node degree of words on the summarization
performance.
We first calculate the average degree of word
nodes for each example based on the constructed
graph. Then the test set of CNN/DailyMail is divided into 5 intervals based on it (x-axis in Figure
4). We evaluate the performance of H ETER SUMG RAPH and Ext-BiLSTM in various parts and the
mean score of R-1, R-2, R-L is drawn as lines
(left y-axis RÃÉ). The ROUGE increases with the
increasing of the average degree of word nodes
in the document, which means that articles with
a high redundancy are easier for neural models to
summarize.
To make ‚àÜRÃÉ between models more obvious, we
draw it with histograms (right y-axis). From Figure 4, we can observe that H ETER SUMG RAPH
performs much better for documents with a higher
average word node degree. This proves that the benefit brought by word nodes lies in the aggregation
of information from sentences and the propagation
of their global representations.
Number of source documents We also investigate how the number of source documents influences the performance of our model. To this end,

6216

RÃÉ

36

Acknowledgment

34

This work was supported by the National Natural
Science Foundation of China (No. U1936214 and
61672162), Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01) and
ZJLab.

32
30

First-3
HSG
HDSG

28
2

3

4

5

6

References

Number of source documents

Figure 5: Relationship between number of source documents (x-axis) and RÃÉ (y-axis).

we divide the test set of Multi-News into different
parts by the number of source documents and discard parts with less than 100 examples. Then, we
take First-3 as the baseline, which concatenates the
top-3 sentences of each source document as the
summary.
In Figure 5, we can observe that the lead baseline raises while both of our model performance
degrade and finally they converge to the baseline.
This is because it is more challenging for models to
extract limited-number sentences that can cover the
main idea of all source documents with the increasing number of documents. However, the First-3
baseline is forced to take sentences from each document which can ensure the coverage. Besides, the
increase of document number enlarges the performance gap between H ETER SUMG RAPH and H ETER D OC SUMG RAPH. This indicates the benefit of
document nodes will become more significant for
more complex document-document relationships.

6

Conclusion

In this paper, we propose a heterogeneous graphbased neural network for extractive summarization.
The introduction of more fine-grained semantic
units in the summarization graph helps our model
to build more complex relationships between sentences . It is also convenient to adapt our singledocument graph to multi-document with document
nodes. Furthermore, our models have achieved
the best results on CNN/DailyMail compared with
non-BERT-based models, and we will take the pretrained language models into account for better
encoding representations of nodes in the future.

Kristjan Arumae and Fei Liu. 2018. Reinforced extractive summarization with question-focused rewards.
In Proceedings of ACL 2018, Student Research
Workshop, pages 105‚Äì111.
Jaime G Carbonell and Jade Goldstein. 1998. The
use of mmr, diversity-based reranking for reordering
documents and producing summaries. In SIGIR, volume 98, pages 335‚Äì336.
Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. In
Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1:
Long Papers), volume 1, pages 484‚Äì494.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.
Yue Dong, Yikang Shen, Eric Crawford, Herke van
Hoof, and Jackie Chi Kit Cheung. 2018. Banditsum: Extractive summarization as a contextual bandit. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,
pages 3739‚Äì3748.
Greg Durrett, Taylor Berg-Kirkpatrick, and Dan Klein.
2016. Learning-based single-document summarization with compression and anaphoricity constraints.
arXiv preprint arXiv:1603.08887.
GuÃànes Erkan and Dragomir R Radev. 2004. Lexrank:
Graph-based lexical centrality as salience in text
summarization. Journal of artificial intelligence research, 22:457‚Äì479.
Alexander Fabbri, Irene Li, Tianwei She, Suyi Li, and
Dragomir Radev. 2019. Multi-news: A large-scale
multi-document summarization dataset and abstractive hierarchical model. In Proceedings of the 57th
Annual Meeting of the Association for Computational Linguistics, pages 1074‚Äì1084, Florence, Italy.
Association for Computational Linguistics.
Sebastian Gehrmann, Yuntian Deng, and Alexander
Rush. 2018. Bottom-up abstractive summarization.
In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages
4098‚Äì4109.

6217

Justin Gilmer, Samuel S Schoenholz, Patrick F Riley,
Oriol Vinyals, and George E Dahl. 2017. Neural
message passing for quantum chemistry. In ICML,
pages 1263‚Äì1272.
Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,
and Phil Blunsom. 2015. Teaching machines to read
and comprehend. In Advances in Neural Information Processing Systems, pages 1684‚Äì1692.
Sepp Hochreiter and JuÃàrgen Schmidhuber. 1997.
Long short-term memory. Neural computation,
9(8):1735‚Äì1780.
Diederik Kingma and Jimmy Ba. 2014. Adam: A
method for stochastic optimization. arXiv preprint
arXiv:1412.6980.
Logan Lebanoff, Kaiqiang Song, and Fei Liu. 2018.
Adapting the neural encoder-decoder framework
from single to multi-document summarization. In
Proceedings of the 2018 Conference on Empirical
Methods in Natural Language Processing, pages
4131‚Äì4141.
Yann LeCun, LeÃÅon Bottou, Yoshua Bengio, Patrick
Haffner, et al. 1998. Gradient-based learning applied to document recognition. Proceedings of the
IEEE, 86(11):2278‚Äì2324.
Chin-Yew Lin and Eduard Hovy. 2003.
Automatic evaluation of summaries using n-gram cooccurrence statistics. In Proceedings of the 2003 Human Language Technology Conference of the North
American Chapter of the Association for Computational Linguistics.
Hu Linmei, Tianchi Yang, Chuan Shi, Houye Ji, and
Xiaoli Li. 2019. Heterogeneous graph attention networks for semi-supervised short text classification.
In Proceedings of the 2019 Conference on Empirical
Methods in Natural Language Processing and the
9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4823‚Äì
4832.
Peter J Liu, Mohammad Saleh, Etienne Pot, Ben
Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam
Shazeer. 2018. Generating wikipedia by summarizing long sequences. Proceedings of the 6th International Conference on Learning Representations.
Yang Liu and Mirella Lapata. 2019a. Hierarchical
transformers for multi-document summarization. In
Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5070‚Äì
5081.
Yang Liu and Mirella Lapata. 2019b. Text summarization with pretrained encoders. In Proceedings of
the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International
Joint Conference on Natural Language Processing
(EMNLP-IJCNLP), pages 3721‚Äì3731, Hong Kong,
China. Association for Computational Linguistics.

Yang Liu, Ivan Titov, and Mirella Lapata. 2019. Single
document summarization as tree induction. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational
Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1745‚Äì1755.
Ling Luo, Xiang Ao, Yan Song, Feiyang Pan, Min
Yang, and Qing He. 2019. Reading like her: Human
reading inspired extractive summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language
Processing (EMNLP-IJCNLP), pages 3024‚Äì3034.
Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language
processing, pages 404‚Äì411.
Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017.
Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Thirty-First AAAI Conference on Artificial
Intelligence.
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
CÃßa glar GulcÃßehre, and Bing Xiang. 2016. Abstractive text summarization using sequence-to-sequence
rnns and beyond. CoNLL 2016, page 280.
Shashi Narayan, Shay B Cohen, and Mirella Lapata.
2018. Ranking sentences for extractive summarization with reinforcement learning. In Proceedings of
the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 1747‚Äì1759.
Romain Paulus, Caiming Xiong, and Richard Socher.
2017. A deep reinforced model for abstractive summarization. arXiv preprint arXiv:1705.04304.
Jeffrey Pennington, Richard Socher, and Christopher
Manning. 2014. Glove: Global vectors for word representation. In Proceedings of the 2014 conference
on empirical methods in natural language processing (EMNLP), pages 1532‚Äì1543.
Evan Sandhaus. 2008. The new york times annotated
corpus. Linguistic Data Consortium, Philadelphia,
6(12):e26752.
Abigail See, Peter J Liu, and Christopher D Manning.
2017. Get to the point: Summarization with pointergenerator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), volume 1,
pages 1073‚Äì1083.
Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and
S Yu Philip. 2016. A survey of heterogeneous information network analysis. IEEE Transactions on
Knowledge and Data Engineering, 29(1):17‚Äì37.

6218

Ming Tu, Guangtao Wang, Jing Huang, Yun Tang, Xiaodong He, and Bowen Zhou. 2019. Multi-hop
reading comprehension across multiple documents
by reasoning over heterogeneous graphs. arXiv
preprint arXiv:1905.07374.

Ming Zhong, Danqing Wang, Pengfei Liu, Xipeng Qiu,
and Xuan-Jing Huang. 2019b. A closer look at data
bias in neural extractive summarization models. In
Proceedings of the 2nd Workshop on New Frontiers
in Summarization, pages 80‚Äì89.

Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing Systems, pages 5998‚Äì6008.

Qingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang,
Ming Zhou, and Tiejun Zhao. 2018. Neural document summarization by jointly learning to score and
select sentences. In Proceedings of the 56th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages
654‚Äì663.

Petar Velickovic, Guillem Cucurull, Arantxa Casanova,
Adriana Romero, Pietro Lio, and Yoshua Bengio.
2017. Graph attention networks. arXiv preprint
arXiv:1710.10903.
Danqing Wang, Pengfei Liu, Ming Zhong, Jie Fu,
Xipeng Qiu, and Xuanjing Huang. 2019a. Exploring
domain shift in extractive text summarization. arXiv
preprint arXiv:1908.11664.
Hsiu-Yi Wang, Jia-Wei Chang, and Jen-Wei Huang.
2019b. User intention-based document summarization on heterogeneous sentence networks. In International Conference on Database Systems for Advanced Applications, pages 572‚Äì587. Springer.
Yang Wei. 2012. Document summarization method
based on heterogeneous graph. In 2012 9th International Conference on Fuzzy Systems and Knowledge
Discovery, pages 1285‚Äì1289. IEEE.

A

In order to select the best iteration number for H ETER SUMG RAPH, we compare performances of different t on the validation set of CNN/DM. All models are trained on a single GeForce RTX 2080 Ti
GPU for about 5 epochs. As Table 5 shows, our
H ETER SUMG RAPH has comparable results for
t = 1 and t = 3. However, when the iteration number goes from 1 to 3, the time for one epoch nearly
doubles. Therefore, we take t = 1 as a result of the
balance of time cost and model performance.
Number
t=0
t=1
t=2
t=3

Jiacheng Xu and Greg Durrett. 2019. Neural extractive text summarization with syntactic compression.
arXiv preprint arXiv:1902.00863.
Jiacheng Xu, Zhe Gan, Yu Cheng, and Jingjing
Liu. 2019.
Discourse-aware neural extractive
model for text summarization.
arXiv preprint
arXiv:1910.14142.

Appendices

R-1

R-2

R-L

Time

43.63
44.26
44.13
44.28

19.58
19.97
19.85
19.96

37.39
38.03
37.87
37.98

3.16h
5.04h
7.20h
8.93h

Table 5: Different turns of iterative updating of sentence nodes. The experiments are performed on the
validation set of CNN/DM. Time is the average time
of one epoch.

Michihiro Yasunaga, Rui Zhang, Kshitijh Meelu,
Ayush Pareek, Krishnan Srinivasan, and Dragomir
Radev. 2017. Graph-based neural multi-document
summarization. arXiv preprint arXiv:1706.06681.
Xingxing Zhang, Mirella Lapata, Furu Wei, and Ming
Zhou. 2018. Neural latent extractive document summarization. In Proceedings of the 2018 Conference
on Empirical Methods in Natural Language Processing, pages 779‚Äì784.
Ming Zhong, Pengfei Liu, Yiran Chen, Danqing Wang,
Xipeng Qiu, and Xuan-Jing Huang. 2020. Extractive summarization as text matching. In Proceedings
of the 58th Conference of the Association for Computational Linguistics.
Ming Zhong, Pengfei Liu, Danqing Wang, Xipeng Qiu,
and Xuan-Jing Huang. 2019a. Searching for effective neural extractive summarization: What works
and what‚Äôs next. In Proceedings of the 57th Annual
Meeting of the Association for Computational Linguistics, pages 1049‚Äì1058.

6219

